{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f43afa",
   "metadata": {
    "id": "ZLfVMQz5rJS5"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f3a03",
   "metadata": {
    "id": "-vAG6dMBsqhI"
   },
   "outputs": [],
   "source": [
    "image_dir = \"/content/drive/MyDrive/Data/Subset_128x128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ab0a89",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "3ctX_ThPsvDZ",
    "outputId": "9eab583a-e4e4-4126-e5ea-8d608cb008f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE IIT BHILAI\\AppData\\Local\\Temp\\ipykernel_2580\\3047473225.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eeg_data = torch.load(r\"EpochHunters/eeg_signals_raw_with_mean_std.pth\")\n"
     ]
    }
   ],
   "source": [
    "eeg_data = torch.load(r\"EpochHunters/eeg_signals_raw_with_mean_std.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd890661",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "3ctX_ThPsvDZ",
    "outputId": "9eab583a-e4e4-4126-e5ea-8d608cb008f5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE IIT BHILAI\\AppData\\Local\\Temp\\ipykernel_2580\\3047473225.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eeg_data = torch.load(r\"EpochHunters/eeg_signals_raw_with_mean_std.pth\")\n"
     ]
    }
   ],
   "source": [
    "eeg_data = torch.load(r\"EpochHunters/eeg_signals_raw_with_mean_std.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad576690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o24IFTv3kDcD",
    "outputId": "f71ed9af-8844-407f-90de-1da939c2b03b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\CSE IIT BHILAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0571b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMIP_O1Es2dB",
    "outputId": "7d96d50d-0045-4fa8-e6ae-2a556d4cb98e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dataset', 'labels', 'images'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf250b3c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKV1DNWmtT_s",
    "outputId": "90dbf365-34d0-4e40-86cd-e701a2386294"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eeg', 'image', 'label', 'subject'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_data[\"dataset\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e29785",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VjG5jGrHticv",
    "outputId": "114d0565-dd59-48d6-9127-4c6fe667e6f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eeg_data[\"dataset\"][0]['eeg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15673f2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcNrowbCt4vA",
    "outputId": "b8aa852f-1334-44c0-d422-0a609090bb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG shape: torch.Size([16, 128, 440])\n",
      "Image shape: torch.Size([16, 3, 128, 128])\n",
      "Labels: tensor([11,  3, 37, 37, 11, 18,  2, 14, 34,  2,  2, 17, 35,  5, 37, 12])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGImageDataset(Dataset):\n",
    "    def __init__(self, eeg_data, image_folder, synset_csv, transform=None, eeg_length=440):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            eeg_data (dict): Dictionary with keys `dataset`, `labels`, `images`.\n",
    "            image_folder (str): Path to the main image folder.\n",
    "            synset_csv (str): Path to the synset CSV file with `images` and `folder` columns.\n",
    "            transform (callable, optional): Optional transform to be applied on the images.\n",
    "            eeg_length (int): Fixed length for the EEG data (e.g., 440).\n",
    "        \"\"\"\n",
    "        self.eeg_data = eeg_data[\"dataset\"]\n",
    "        self.image_folder = image_folder\n",
    "        self.synset_df = pd.read_csv(synset_csv)\n",
    "        self.transform = transform\n",
    "        self.eeg_length = eeg_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eeg_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        eeg_entry = self.eeg_data[idx]\n",
    "        eeg_signal = eeg_entry[\"eeg\"].to(torch.float32)  # Shape: (128, variable-length)\n",
    "\n",
    "        \n",
    "        if eeg_signal.shape[1] < self.eeg_length:  \n",
    "            pad_size = self.eeg_length - eeg_signal.shape[1]\n",
    "            eeg_signal = F.pad(eeg_signal, (0, pad_size), mode='constant', value=0)\n",
    "        elif eeg_signal.shape[1] > self.eeg_length:  \n",
    "            eeg_signal = eeg_signal[:, :self.eeg_length]\n",
    "\n",
    "        \n",
    "        image_index = eeg_entry[\"image\"]\n",
    "        label = eeg_entry[\"label\"]\n",
    "        image_filename = self.synset_df.iloc[image_index][\"images\"] + \".JPEG\"\n",
    "        folder_name = self.synset_df.iloc[image_index][\"folder\"]\n",
    "        image_path = os.path.join(self.image_folder, folder_name, image_filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"eeg\": eeg_signal, \"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "\n",
    "# Define transformations for the images \n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "\n",
    "image_folder_path = \"/content/drive/MyDrive/Data/Subset_128x128\"\n",
    "synset_csv_path = \"/content/drive/MyDrive/Data/synsetIds.csv\"\n",
    "\n",
    "\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "eeg_image_dataset = EEGImageDataset(\n",
    "    eeg_data=eeg_data,\n",
    "    image_folder=image_folder_path,\n",
    "    synset_csv=synset_csv_path,\n",
    "    transform=image_transforms\n",
    ")\n",
    "\n",
    "# Create a DataLoader \n",
    "data_loader = DataLoader(eeg_image_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "for batch in data_loader:\n",
    "    print(\"EEG shape:\", batch[\"eeg\"].shape)  # Shape: (batch_size, 128, 440)\n",
    "    print(\"Image shape:\", batch[\"image\"].shape)  # Shape: (batch_size, 3, 128, 128)\n",
    "    print(\"Labels:\", batch[\"label\"])  # Shape: (batch_size,)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7132ab",
   "metadata": {
    "id": "bEQT87MrwrxR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c94c8f",
   "metadata": {
    "id": "uACCgh3f17jh"
   },
   "source": [
    "## Using both for Classification and Generation Simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53341be",
   "metadata": {
    "id": "yVDK3B6i0L2h"
   },
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim=40, num_classes=40):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.fc(x), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de5a5e",
   "metadata": {
    "id": "_-AZa8Qo0QR9"
   },
   "outputs": [],
   "source": [
    "class GenerationHead(nn.Module):\n",
    "    def __init__(self, input_dim=40, output_dim=128):\n",
    "        super(GenerationHead, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5321b9b",
   "metadata": {
    "id": "u16rWLVx0Rps"
   },
   "outputs": [],
   "source": [
    "class EEGModel(nn.Module):\n",
    "    def __init__(self, noise_dim=100, conditional_dim=128):\n",
    "        super(EEGModel, self).__init__()\n",
    "        # EEG Encoder\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(128, 40)  # Output of 40-dimensional latent feature\n",
    "\n",
    "        # Classification Head\n",
    "        self.classification_head = nn.Linear(40, 40)  # Map to latent to class logits\n",
    "\n",
    "        # Generation Head (upsample )\n",
    "        self.generation_head = nn.Linear(40, conditional_dim)\n",
    "\n",
    "        # Noise dimension for generator\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "    def forward(self, eeg):\n",
    "        # Transpose EEG input to match the LSTM shape\n",
    "        eeg = eeg.transpose(1, 2)  # Change the shape from the (batch_size, 128, 440) to the(batch_size, 440, 128)\n",
    "\n",
    "        \n",
    "        lstm_out, _ = self.lstm(eeg)  # LSTM outputs of the (batch_size, 440, 128)\n",
    "        latent = self.encoder_fc(lstm_out[:, -1, :])  \n",
    "\n",
    "        \n",
    "        classification_output = self.classification_head(latent)\n",
    "\n",
    "        \n",
    "        conditional_output = self.generation_head(latent)\n",
    "\n",
    "        return classification_output, conditional_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb4f5dd",
   "metadata": {
    "id": "TZFTBntr0Usu"
   },
   "outputs": [],
   "source": [
    "class ResBlockUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlockUp, self).__init__()\n",
    "        # Main path\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        # Shortcut path\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        main_path = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        main_path = self.block(main_path)\n",
    "\n",
    "        \n",
    "        shortcut_path = self.upsample(x)\n",
    "\n",
    "        \n",
    "        return main_path + shortcut_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, conditional_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(noise_dim + conditional_dim, 4 * 4 * 1024)\n",
    "        self.resblock1 = ResBlockUp(1024, 512)\n",
    "        self.resblock2 = ResBlockUp(512, 256)\n",
    "        self.resblock3 = ResBlockUp(256, 128)\n",
    "        self.resblock4 = ResBlockUp(128, 64)\n",
    "        self.conv = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, condition):\n",
    "        x = torch.cat([noise, condition], dim=1)\n",
    "        x = self.fc(x).view(-1, 1024, 4, 4)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.conv(x)\n",
    "        return self.tanh(x)\n",
    "\n",
    "\n",
    "class ResBlockDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ResBlockDown, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        self.downsample = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        main_path = self.block(x)\n",
    "\n",
    "        \n",
    "        shortcut_path = self.downsample(x)\n",
    "\n",
    "        \n",
    "        return main_path + shortcut_path\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, conditional_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.resblock1 = ResBlockDown(3, 64)\n",
    "        self.resblock2 = ResBlockDown(64, 128)\n",
    "        self.resblock3 = ResBlockDown(128, 256)\n",
    "        self.resblock4 = ResBlockDown(256, 512)\n",
    "        self.resblock5 = ResBlockDown(512, 1024)\n",
    "        self.resblock_final = ResBlockDown(1024, 1024)\n",
    "        self.global_sum_pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "        self.embed = nn.Linear(conditional_dim, 1024)\n",
    "\n",
    "    def forward(self, img, condition):\n",
    "        x = self.resblock1(img)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock_final(x)\n",
    "        x = self.global_sum_pooling(x).view(x.size(0), -1)\n",
    "        condition_embed = self.embed(condition)\n",
    "        x = x + condition_embed\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fe413",
   "metadata": {
    "id": "yXajrIZH0ap9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3541b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsUveJRh01Cf",
    "outputId": "78134c73-b218-4b56-8918-f393c902cec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7e8b3",
   "metadata": {
    "id": "tvwMFXKm05A5"
   },
   "outputs": [],
   "source": [
    "eeg_model = EEGModel().to(device)\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39649cd8",
   "metadata": {
    "id": "pmshHpYj5txG"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2729b6",
   "metadata": {
    "id": "8vGo77dr1P34"
   },
   "outputs": [],
   "source": [
    "def train_model(eeg_model, generator, discriminator, data_loader, num_epochs=20, lr=0.0002):\n",
    "    # Optimizers\n",
    "    optimizer_eeg = torch.optim.Adam(eeg_model.parameters(), lr=lr)\n",
    "    optimizer_gen = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "    \n",
    "    classification_loss_fn = nn.CrossEntropyLoss()\n",
    "    gan_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        \n",
    "        batch_iterator = tqdm(data_loader, desc=f\"Training Progress for Epoch {epoch + 1}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "            eeg, images, labels = batch[\"eeg\"].to(device), batch[\"image\"].to(device), batch[\"label\"].to(device)\n",
    "\n",
    "            #  Classification Training \n",
    "            optimizer_eeg.zero_grad()\n",
    "            class_output, condition = eeg_model(eeg)\n",
    "            class_loss = classification_loss_fn(class_output, labels)\n",
    "            class_loss.backward()\n",
    "            optimizer_eeg.step()\n",
    "\n",
    "            \n",
    "            condition = condition.detach()\n",
    "\n",
    "            # GAN Training \n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_disc.zero_grad()\n",
    "            noise = torch.randn(eeg.size(0), 100).to(device)\n",
    "            fake_images = generator(noise, condition)  # Generate fake images\n",
    "\n",
    "            real_labels = torch.ones(eeg.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(eeg.size(0), 1).to(device)\n",
    "\n",
    "            # Discriminator Loss\n",
    "            real_loss = gan_loss_fn(discriminator(images, condition), real_labels)\n",
    "            fake_loss = gan_loss_fn(discriminator(fake_images.detach(), condition), fake_labels)\n",
    "            disc_loss = real_loss + fake_loss\n",
    "            disc_loss.backward()\n",
    "            optimizer_disc.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_gen.zero_grad()\n",
    "            noise = torch.randn(eeg.size(0), 100).to(device)  # Regenerate noise\n",
    "            fake_images = generator(noise, condition)  # Re-generate fake images\n",
    "            gen_loss = gan_loss_fn(discriminator(fake_images, condition), real_labels)\n",
    "            gen_loss.backward()  # Backprop through generator\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            \n",
    "            batch_iterator.set_postfix({\n",
    "                \"Class Loss\": f\"{class_loss.item():.4f}\",\n",
    "                \"Gen Loss\": f\"{gen_loss.item():.4f}\",\n",
    "                \"Disc Loss\": f\"{disc_loss.item():.4f}\"\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a5d41",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3V5ksHpS1Sye",
    "outputId": "5f371985-f836-4b95-b4ed-006f1d886549",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 1: 100%|██████████| 748/748 [05:49<00:00,  2.14it/s, Class Loss=3.6168, Gen Loss=15.8167, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 2: 100%|██████████| 748/748 [03:25<00:00,  3.65it/s, Class Loss=3.3963, Gen Loss=17.7395, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 3: 100%|██████████| 748/748 [03:24<00:00,  3.66it/s, Class Loss=2.9861, Gen Loss=18.0463, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 4: 100%|██████████| 748/748 [03:23<00:00,  3.67it/s, Class Loss=2.7350, Gen Loss=18.0913, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 5: 100%|██████████| 748/748 [03:23<00:00,  3.67it/s, Class Loss=2.8413, Gen Loss=18.5658, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 6: 100%|██████████| 748/748 [03:23<00:00,  3.68it/s, Class Loss=3.1035, Gen Loss=18.5965, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 7: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.4753, Gen Loss=19.0895, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 8: 100%|██████████| 748/748 [03:21<00:00,  3.70it/s, Class Loss=2.4877, Gen Loss=19.4802, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 9: 100%|██████████| 748/748 [03:23<00:00,  3.68it/s, Class Loss=2.4819, Gen Loss=19.2023, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 10: 100%|██████████| 748/748 [03:22<00:00,  3.68it/s, Class Loss=2.8244, Gen Loss=20.4525, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 11: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.0887, Gen Loss=21.7873, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 12: 100%|██████████| 748/748 [03:23<00:00,  3.68it/s, Class Loss=2.1755, Gen Loss=21.7640, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 13: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.4676, Gen Loss=23.1112, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 14: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.1273, Gen Loss=23.4995, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 15: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.1596, Gen Loss=25.0907, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 16: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.0398, Gen Loss=24.9438, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 17: 100%|██████████| 748/748 [03:23<00:00,  3.68it/s, Class Loss=2.1737, Gen Loss=24.2970, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 18: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.1601, Gen Loss=25.8628, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 19: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.5103, Gen Loss=25.9327, Disc Loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress for Epoch 20: 100%|██████████| 748/748 [03:22<00:00,  3.69it/s, Class Loss=2.0690, Gen Loss=26.0613, Disc Loss=0.0000]\n"
     ]
    }
   ],
   "source": [
    "train_model(eeg_model, generator, discriminator, data_loader, num_epochs=20, lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da7142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
